# lit

This is a collection of technical books and papers that I've enjoyed. I can't guarantee that all of these books have been acquired in a lawful way. Have fun.

## Books

- math/stats
    - [ ] Mathematics for Machine Learning
    - [ ] The Princeton Companion to Mathematics
    - [ ] Understanding Digital Signal Processing
    - [X] **Introduction to Mathematical Statistics**
    - [X] Doing Math with Python
    - [X] **Introduction to Statistics with Python**
    - [ ] Elements of Information Theory
    - [ ] Mathematical Statistics and Data Analysis
    - [ ] Probability and Random Variables
    - [ ] Bayesian Reasoning and Machine Learning

- artificial intelligence
    - [X] NLTK Book
    - [X] **Handbook of Natural Language Processing**
    - [X] **Deep Learning Book**
    - [X] **Speech and Language Processing** (https://web.stanford.edu/~jurafsky/slp3/)
    - [X] Zero to Deep Learning
    - [ ] Artificial Intelligence: A Modern Approach
    - [ ] Elements of Statistical Learning
    - [ ] Introduction to Machine Learning
    - [ ] Reinforcement Learning: An Introduction

- data science
    - [X] **Python for Data Analysis**
    - [X] **Hands on Machine Learning**
    - [ ] Python End-to-End Data Analysis
    - [ ] Doing Data Science
    - [ ] SQL Cookbook
    - [ ] Data Science and Complex Networks

- networks
    - [X] Flask Web Development
    - [ ] CCNA Electronic Book
    - [ ] Hacking: The Art of Exploitation
    - [ ] Internet Routing Architectures
    - [ ] Communications and Networking

- computer science
    - [X] Code
    - [X] **Grokking Algorithms**
    - [X] **Cracking the Coding Interview**
    - [ ] Data Structures and Algorithms in Python
    - [ ] The Structure and Interpretation of Computer Programs
    - [ ] Introduction to Algorithms
    - [ ] Programming Languages: Application and Interpretation

- operating systems
    - [X] Unix for Poets
    - [X] How Linux Works

- programming
    - [X] Mastering Python Regular Expressions
    - [X] Effective Python
    - [ ] Accelerated C++
    - [ ] JavaScript: the Definitive Guide

## Papers

### Embeddings

A simple but hard-to-beat baseline for sentence embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma (2017). [link](https://openreview.net/pdf?id=SyK00v5xx)

Describes how taking a weighted average of the word vectors in a sentence can produce high-quality sentence embeddings.s

<hr>

Learning without forgetting. Zhizhong Li, Derek Hoiem (2016). [link](http://zli115.web.engr.illinois.edu/wp-content/uploads/2016/10/0479.pdf)

Describes different strategies for transfer learning: fine-tuning, feature extraction, joint training, and the paper's new strategy: "learning without forgetting". This tries to solve the problem of "continually adding new prediction tasks based on adapting shared parameters without access to training data for previously learned tasks"

<hr>

Deep Face Recognition: A Survey. Mei Wang, Weihong Deng (2019). [link](https://arxiv.org/pdf/1804.06655.pdf)

<hr>

Learning Multi-scale Features for Foreground Segmentation. Long Ang Lim, Hacer Yalim Keles (2018). [link](https://arxiv.org/pdf/1808.01477.pdf)

<hr>

Xception: Deep Learning with Depthwise Separable Convolutions. Francois Chollet (2017). [link](https://arxiv.org/pdf/1610.02357.pdf)

<hr>

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Sergey Ioffe, Christian Szegedy (2015). [link](https://arxiv.org/pdf/1502.03167.pdf)

<hr>

Universal Language Model Fine-tuning for Text Classification. Jeremy Howard, Sebastian Ruder (2018). [link](https://aclweb.org/anthology/P18-1031)

<hr>

Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov (2014). [link](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)

<hr>

The Unreasonable Effectiveness of Recurrent Neural Networks. Andrej Karpathy (2015). [link](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

<hr>

How transferable are features in deep neural networks? Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson (2014). [link](https://arxiv.org/pdf/1411.1792.pdf)

<hr>

Patent Claim Generation by Fine-Tuning OpenAI GPT-2. Jieh-Sheng Lee and Jieh Hsiang (2019). [link](https://arxiv.org/pdf/1907.02052.pdf)

<hr>

Deep Facial Expression Recognition: A Survey. Shan Li and Weihong Deng (2018). [link](https://arxiv.org/pdf/1804.08348.pdf)

<hr>

Challenges in Representation Learning: A report on three machine learning contests. Goodfellow et al (2013). [link](https://arxiv.org/pdf/1307.0414.pdf)

<hr>

An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models. Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos (2019). [link](https://arxiv.org/pdf/1902.10547.pdf)

<hr>

Attention Is All You Need. Ashish Vaswani et al (2017). [link](https://arxiv.org/pdf/1706.03762.pdf)

<hr>

A Selective Overview of Deep Learning. Jianqing Fan, Cong Ma, Yiqiao Zhong (2019). [link](https://arxiv.org/pdf/1904.05526.pdf)

<hr>

Self-Normalizing Neural Networks. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter (2017). [link](https://arxiv.org/pdf/1706.02515.pdf)

<hr>

What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? Alex Kendall, Yarin Gal (2017). [link](https://arxiv.org/pdf/1703.04977.pdf)

<hr>

Learning to Reinforcement Learn. Wang et al (2017). [link](https://arxiv.org/pdf/1611.05763.pdf)

<hr>

Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (2019). [link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

<hr>

Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP Tasks. Steffen Eger, Paul Youssef, Iryna Gurevych (2018). [link](https://www.aclweb.org/anthology/D18-1472)

<hr>

Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015). [link](https://arxiv.org/pdf/1512.03385.pdf)

<hr>

YOLOv3: An Incremental Improvement. Joseph Redmon, Ali Farhadi (2018). [link](https://arxiv.org/pdf/1804.02767.pdf)

<hr>

Universal Transformers. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser (2019). [link](https://arxiv.org/pdf/1807.03819.pdf)

<hr>

R-net: Machine Reading Comprehension with Self-Matching Networks. Natural Language Computing Group, Microsoft Research Asia (2017). [link](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf)

<hr>

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2019). [link](https://arxiv.org/pdf/1810.04805.pdf)

<hr>

Improving Language Understanding by Generative Pre-Training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (2018). [link](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

<hr>

Deep contextualized word representations. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer (2018). [link](https://arxiv.org/pdf/1802.05365.pdf)

<hr>

ImageNet: A Large-Scale Hierarchical Image Database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei (2009) [link](http://www.image-net.org/papers/imagenet_cvpr09.pdf)

<hr>

ImageNet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton (2013). [link](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)

<hr>

A State-of-the-Art Survey on Deep Learning Theory and Architectures. Md Zahangir Alom et al (2019). [link](https://www.mdpi.com/2079-9292/8/3/292/htm)

<hr>

Understanding deep learning requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals (2017). [link](https://arxiv.org/pdf/1611.03530.pdf)

<hr>

A Closer Look at Memorization in Deep Networks. Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien (2017). [link](https://arxiv.org/pdf/1706.05394.pdf)

<hr>

Probing Biomedical Embeddings from Language Models. Qiao Jin, Bhuwan Dhingra, William W. Cohen, Xinghua Lu (2019). [link](https://www.aclweb.org/anthology/W19-2011)

<hr>

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra (2017). [link](https://arxiv.org/pdf/1610.02391.pdf).

<hr>

CNN Features off-the-shelf: an Astounding Baseline for Recognition. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson (2014). [link](https://arxiv.org/pdf/1403.6382.pdf)

<hr>

Know What You Don't Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang (2018). [link](https://arxiv.org/pdf/1806.03822.pdf)

<hr>

An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Shaojie Bai, J. Zico Kolter, Vladlen Koltun (2018). [link](https://arxiv.org/pdf/1803.01271.pdf)
