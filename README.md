# lit

This is a collection of technical books and papers that I've enjoyed. I can't guarantee that all of these books have been acquired in a lawful way. Have fun.

## Books

- math/stats
    - [ ] The Princeton Companion to Mathematics
    - [X] **Introduction to Mathematical Statistics**
    - [X] Linear Algebra Done Right
    - [ ] **Doing Math with Python**
    - [ ] **Introduction to Statistics with Python**
    - [ ] Elements of Information Theory
    - [ ] Mathematical Statistics and Data Analysis
    - [ ] Probability and Random Variables
    - [ ] Bayesian Reasoning and Machine Learning
- artificial intelligence
    - [X] NLTK Book
    - [X] **Hands on Machine Learning**
    - [X] **Handbook of Natural Language Processing**
    - [X] **Deep Learning Book**
    - [X] **Speech and Language Processing** (https://web.stanford.edu/~jurafsky/slp3/)
    - [X] Zero to Deep Learning
    - [ ] Artificial Intelligence: A Modern Approach
    - [ ] Elements of Statistical Learning
    - [ ] Programming Collective Intelligence
    - [ ] Path to Predictive Analytics and Machine Learning
    - [ ] Introduction to Machine Learning
    - [ ] Building Machine Learning Systems with Python
    - [ ] Reinforcement Learning: An Introduction
    - [ ] Think Python
- data science
    - [ ] **Python for Data Analysis**
    - [ ] Python End-to-End Data Analysis
    - [ ] **Doing Data Science**
    - [ ] Data Science at the Command Line
    - [ ] SQL Cookbook
    - [ ] Web Scraping with Python
    - [ ] Building Real Time Data Pipelines
    - [ ] Data Science from Scratch
    - [ ] Building Real Time Data Platforms
    - [ ] Data Analysis Using Regression
    - [ ] Data Science and Complex Networks
    - [ ] Doing Data Analysis
- networks
    - [X] Flask Web Development
    - [ ] Violent Python
    - [ ] CCNA Electronic Book
    - [ ] Hacking: The Art of Exploitation
    - [ ] Internet Routing Architectures
    - [ ] Communications and Networking
- computer science
    - [X] Code
    - [X] **Grokking Algorithms**
    - [X] **Cracking the Coding Interview**
    - [ ] The Structure and Interpretation of Computer Programs
    - [ ] Introduction to Algorithms
    - [ ] Programming Languages: Application and Interpretation
- operating systems
    - [X] Unix for Poets
    - [X] How Linux Works
    - [ ] The Linux Programming Interface
- programming
    - [X] Mastering Python Regular Expressions
    - [X] **Effective Python**
    - [ ] Fluent Python
    - [ ] Python Cookbook
    - [ ] Learning Python
    - [ ] Accelerated C++
    - [ ] JavaScript: the Definitive Guide

## Papers

Learning without forgetting. Zhizhong Li, Derek Hoiem (2016). [link](http://zli115.web.engr.illinois.edu/wp-content/uploads/2016/10/0479.pdf)

Describes different strategies for transfer learning: fine-tuning, feature extraction, joint training, and the paper's new strategy: "learning without forgetting". This tries to solve the problem of "continually adding new prediction tasks based on adapting shared parameters without access to training data for previously learned tasks"

<hr>

Deep Face Recognition: A Survey. Mei Wang, Weihong Deng (2019). [link](https://arxiv.org/pdf/1804.06655.pdf)

<hr>

Learning Multi-scale Features for Foreground Segmentation. Long Ang Lim, Hacer Yalim Keles (2018). [link](https://arxiv.org/pdf/1808.01477.pdf)

<hr>

Xception: Deep Learning with Depthwise Separable Convolutions. Francois Chollet (2017). [link](https://arxiv.org/pdf/1610.02357.pdf)

<hr>

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Sergey Ioffe, Christian Szegedy (2015). [link](https://arxiv.org/pdf/1502.03167.pdf)

<hr>

Universal Language Model Fine-tuning for Text Classification. Jeremy Howard, Sebastian Ruder (2018). [link](https://aclweb.org/anthology/P18-1031)

<hr>

Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov (2014). [link](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)

<hr>

The Unreasonable Effectiveness of Recurrent Neural Networks. Andrej Karpathy (2015). [link](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

<hr>

How transferable are features in deep neural networks? Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson (2014). [link](https://arxiv.org/pdf/1411.1792.pdf)

<hr>

Patent Claim Generation by Fine-Tuning OpenAI GPT-2. Jieh-Sheng Lee and Jieh Hsiang (2019). [link](https://arxiv.org/pdf/1907.02052.pdf)

<hr>

Deep Facial Expression Recognition: A Survey. Shan Li and Weihong Deng (2018). [link](https://arxiv.org/pdf/1804.08348.pdf)

<hr>

Challenges in Representation Learning: A report on three machine learning contests. Goodfellow et al (2013). [link](https://arxiv.org/pdf/1307.0414.pdf)

<hr>

An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models. Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos (2019). [link](https://arxiv.org/pdf/1902.10547.pdf)

<hr>

Attention Is All You Need. Ashish Vaswani et al (2017). [link](https://arxiv.org/pdf/1706.03762.pdf)

<hr>

A Selective Overview of Deep Learning. Jianqing Fan, Cong Ma, Yiqiao Zhong (2019). [link](https://arxiv.org/pdf/1904.05526.pdf)

<hr>

Self-Normalizing Neural Networks. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter (2017). [link](https://arxiv.org/pdf/1706.02515.pdf)

<hr>

What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? Alex Kendall, Yarin Gal (2017). [link](https://arxiv.org/pdf/1703.04977.pdf)

<hr>

Learning to Reinforcement Learn. Wang et al (2017). [link](https://arxiv.org/pdf/1611.05763.pdf)

<hr>

Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (2019). [link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

<hr>

Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP Tasks. Steffen Eger, Paul Youssef, Iryna Gurevych (2018). [link](https://www.aclweb.org/anthology/D18-1472)

<hr>

Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015). [link](https://arxiv.org/pdf/1512.03385.pdf)

<hr>

YOLOv3: An Incremental Improvement. Joseph Redmon, Ali Farhadi (2018). [link](https://arxiv.org/pdf/1804.02767.pdf)

<hr>

Universal Transformers. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser (2019). [link](https://arxiv.org/pdf/1807.03819.pdf)

<hr>

R-net: Machine Reading Comprehension with Self-Matching Networks. Natural Language Computing Group, Microsoft Research Asia (2017). [link](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf)

<hr>

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2019). [link](https://arxiv.org/pdf/1810.04805.pdf)

<hr>

ImageNet: A Large-Scale Hierarchical Image Database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei (2009) [link](http://www.image-net.org/papers/imagenet_cvpr09.pdf)

<br>

ImageNet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton (2013). [link](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)

<br>

A State-of-the-Art Survey on Deep Learning Theory and Architectures. Md Zahangir Alom et al (2019). [link](https://www.mdpi.com/2079-9292/8/3/292/htm)

<br>

Understanding deep learning requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals (2017). [link](https://arxiv.org/pdf/1611.03530.pdf)

<br>

A Closer Look at Memorization in Deep Networks. Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien (2017). [link](https://arxiv.org/pdf/1706.05394.pdf)

<br>

Probing Biomedical Embeddings from Language Models. Qiao Jin, Bhuwan Dhingra, William W. Cohen, Xinghua Lu (2019). [link](https://www.aclweb.org/anthology/W19-2011)

<br>

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra (2017). [link](https://arxiv.org/pdf/1610.02391.pdf).

<br>

CNN Features off-the-shelf: an Astounding Baseline for Recognition. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson (2014). [link](https://arxiv.org/pdf/1403.6382.pdf)
